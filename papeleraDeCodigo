


from sklearn.utils import check_random_state, shuffle
import matplotlib.cm as cm



def hazAlgoConEsto(clustering):#BSD http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_stability_low_dim_dense.html#sphx-glr-auto-examples-cluster-plot-kmeans-stability-low-dim-dense-py
    random_state = np.random.RandomState(0)

    # Number of run (with randomly generated dataset) for each strategy so as
    # to be able to compute an estimate of the standard deviation
    n_runs = 5

    # k-means models can do several random inits so as to be able to trade
    # CPU time for convergence robustness
    n_init_range = np.array([1, 5, 10, 15, 20])

    # Datasets generation parameters
    n_samples_per_center = 100
    grid_size = 3
    scale = 0.1
    n_clusters = grid_size ** 2


    def make_data(random_state, n_samples_per_center, grid_size, scale):
        random_state = check_random_state(random_state)
        centers = np.array([[i, j]
                            for i in range(grid_size)
                            for j in range(grid_size)])
        n_clusters_true, n_features = centers.shape

        noise = random_state.normal(
            scale=scale, size=(n_samples_per_center, centers.shape[1]))

        X = np.concatenate([c + noise for c in centers])
        y = np.concatenate([[i] * n_samples_per_center
                            for i in range(n_clusters_true)])
        return shuffle(X, y, random_state=random_state)
    X, y = make_data(random_state, n_samples_per_center, grid_size, scale)
    fig = plt.figure()
    for k in range(8):
        my_members = clustering.labels_ == k
        color = cm.spectral(float(k) / 8, 1)
        plt.plot(X[my_members, 0], X[my_members, 1], 'o', marker='.', c=color)
        cluster_center = clustering.cluster_centers_[k]
        plt.plot(cluster_center[0], cluster_center[1], 'o',
                 markerfacecolor=color, markeredgecolor='k', markersize=6)
        plt.title("Example cluster allocation with a single random init\n"
                  "with MiniBatchKMeans")
    plt.show()








def getClusterFromGroup(grupoPropio,listaDeEtiquetas):
    grupo=-1
    print clustering.labels__[5]
    for i in clustering.labels_:
        grupo=grupo+1
        #if(grupo==grupoPropio)
    return grupo







"""imports RandomForest
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import GroupKFold
    from sklearn.cross_validation import KFold
"""



def estimador(comida,data):
    if (comida>0):
        return data['Carbohidratos'].median()
    else:
        return 0


"""drop de 'grupo' y clustering
    Esto deberia dropear la columa grupos pero no lo hace
    for index,grupo in data_agrupada:
        grupo=grupo.drop('Grupo',axis=1)
        print index
        print grupo
    print data_agrupada.get_group(82)
    cluster=KMeans()
    cluster.fit(data_agrupada)
"""


print "Prediciendo..."
o = alg.predict(output).astype(int)

print('Imprimiendo...')
submission = pd.DataFrame({
        "date": output["date"],
        "Historico": o
    })
submission.to_csv("output.csv", index = False)


print "Preparando IA..."
predictors=["date"]
target=["Historico"]
kf = KFold(data.shape[0], random_state=1)
alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)



"""CONVERTIR GROUPBY A ARRAY
grupos=[]
for index,grupo in data_agrupada:
    grupo=grupo.drop('Grupo',axis=1)
    grupos.append(grupo)
print grupos[0]
grupos=np.array(grupos, dtype=object)
cluster=KMeans()
#cluster.fit(data)
"""




"""
parser inicial
  columnas=['Hora','Tipo','Historico','Leida']
  data=pd.read_table('../csv.txt',header = 1, usecols=[1,2,3,4],names=columnas,parse_dates='Hora')
  print "Realizando adaptaciones pertinentes..."
  print "Esto puede tardar unos segundos"
  format="%Y/%m/%d %H:%M"
  primeraHora = data['Hora'].min()
  data['Grupo']=data['Hora'].map(lambda x: clasificaPorHora(x,primeraHora,format))
  """


gkf = GroupKFold(n_splits=3)

alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=8, min_samples_leaf=4)
# Compute the accuracy score for all the cross-validation folds; this is much simpler than what we did before
scores = cross_validation.cross_val_score(alg, data[predictors], data["Historico"], cv=3)

# Take the mean of the scores (because we have one for each fold)
print(scores.mean())

#output
columnas=['Hora']
output=pd.read_csv('../out.csv',header = 0, usecols=[0,1],names=columnas,parse_dates=True)
output['Grupo']=output['Hora'].map(lambda x: clasificaPorHora(x,"2016/03/30 16:30"))
output["date"]=output['Hora'].map(lambda x: pd.to_datetime(x,format=format, errors='ignore'))
output=output.drop('Hora',axis=1)
#output.groupby(by="Grupo").mean
#predicciones
train_data= data.values
test_data= output.values
predictors=["date","Tipo","Grupo"]
target=["Historico"]
# Initialize our algorithm class
alg = RandomForestClassifier(random_state=1, n_estimators=390, min_samples_split=16, min_samples_leaf=4)
# We set random_state to ensure we get the same splits every time we run this.
kf = KFold(data.shape[0], n_folds=3, random_state=1)
predictions = []
for train, test in kf:
    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.
    train_predictors = (data[predictors].iloc[train,:])
    # The target we're using to train the algorithm.
    train_target = data["Historico"].iloc[train]
    # Training the algorithm using the predictors and target.
    alg.fit(train_predictors, train_target)
    # We can now make predictions on the test fold
    test_predictions = alg.predict(data[predictors].iloc[test,:])
    predictions.append(test_predictions)
predictions = np.concatenate(predictions, axis=0)
print('imprimimos salida')
# submission--toCSV
submission = pandas.DataFrame({
        "date": output["date"],
        "Tipo": output["Tipo"],
        "Historico": predictions
    })
submission.to_csv("../put_out.csv", index = False)
