"""
def toPDF(clusters,codes):
    pdf=FPDF('P','mm','A4')
    pdf.add_page()
    pdf.set_font('Times','',14)
    texto="A continuacion le mostraremos el analis que hemos realizado con los datos que nos ha entregado.\n"
    texto=texto+"Los resultados de estos analisis son el producto de la aplicacion de tecnicas de aprendizaje automatico\n"
    texto+="Bajo ninguna circunstancia deberan sustituir a los consejo de su medico\n"
    texto+="Ni la UCM, ni el grupo de investigacion se haran responsables del uso indevido que pueda hacerse con estos datos\n"
    pdf.cell(100,10,texto,1)
    pdf.output('../tuto.pdf','F')
"""



  silhouette_avg = silhouette_score(data, clusterer.labels_)
  cal_score = calinski_harabaz_score(data,clusterer.labels_)
  print("For n_clusters =", _nClusters-1,
        "The average silhouette_score is :", silhouette_avg,
        ", the calinski_harabaz score is ", cal_score)


    data=filtro(data,0)

  """
  if (hora_==23 and min_>53):
      codigo=6
  elif(hora_<4 or (hora_==4 and min_<7)):
      codigo=0
  elif(hora_<8 or (hora_==8 and min_<7)):
      codigo=1
  elif(hora_<12 or ((hora_==12) and min_<7)):
      codigo=2
  elif(hora_<16 or ((hora_==16) and min_<7)):
      codigo=3
  elif(hora_<20 or ((hora_==20) and min_<7)):
      codigo=4
  elif(hora_<24 or ((hora_==00) and min_<7)):
      codigo=5
  else:
      codigo=9
      print "ERROR"
  """


def get_group(hora,hMin,format):
    group = 0
    hora_Actual=datetime.strptime(hora,format)
    hora_minima=datetime.strptime(hMin,format)
    hora_minima.day

main"""
  metric=False
  cluste=False
  _nCluster=False
  cluster="kmeans"
  #metrica

  for param,index in zip(sys.argv,range(len(sys.argv))):
      if (index==1):
          archivo=param
      el
  if (len(sys.argv)>1):
      archivo=sys.argv[1]
      ""
      if (len(sys.argv>2)):
          metrica=sys


          tecnicaClustering=sys.argv[2]
          if (tecnicaClustering=='kmeans'):
      ""
      data=getRegistros0(parser(archivo))
      #data=rellenaUnSoloHueco(data)
      #calculaPendiente(data)
      procesado(data)
  """



  """
  plot = getPlot(clusters)
  #plot.show()
  plot.figure().savefig('../'+sys.argv[1]+'_graficas.png')
  plot.close()
  ""
  for i in datos:
      print i
  """



      """
      #data_pendiente.append(grupo.Pendiente)
  else:
      #print len(grupo['Historico'])
      if (len(grupo['Historico'])==14):
          print grupo
#print len(data_agrupada)
"""
#infoKMeans(data_agrupada,data_agrupada)

elif (tipo==1):
    col=['Leida','Hora','Grupo']
    data=data[col]
    data=data.dropna(axis=0)


from sklearn.utils import check_random_state, shuffle
import matplotlib.cm as cm



def hazAlgoConEsto(clustering):#BSD http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_stability_low_dim_dense.html#sphx-glr-auto-examples-cluster-plot-kmeans-stability-low-dim-dense-py
    random_state = np.random.RandomState(0)

    # Number of run (with randomly generated dataset) for each strategy so as
    # to be able to compute an estimate of the standard deviation
    n_runs = 5

    # k-means models can do several random inits so as to be able to trade
    # CPU time for convergence robustness
    n_init_range = np.array([1, 5, 10, 15, 20])

    # Datasets generation parameters
    n_samples_per_center = 100
    grid_size = 3
    scale = 0.1
    n_clusters = grid_size ** 2


    def make_data(random_state, n_samples_per_center, grid_size, scale):
        random_state = check_random_state(random_state)
        centers = np.array([[i, j]
                            for i in range(grid_size)
                            for j in range(grid_size)])
        n_clusters_true, n_features = centers.shape

        noise = random_state.normal(
            scale=scale, size=(n_samples_per_center, centers.shape[1]))

        X = np.concatenate([c + noise for c in centers])
        y = np.concatenate([[i] * n_samples_per_center
                            for i in range(n_clusters_true)])
        return shuffle(X, y, random_state=random_state)
    X, y = make_data(random_state, n_samples_per_center, grid_size, scale)
    fig = plt.figure()
    for k in range(8):
        my_members = clustering.labels_ == k
        color = cm.spectral(float(k) / 8, 1)
        plt.plot(X[my_members, 0], X[my_members, 1], 'o', marker='.', c=color)
        cluster_center = clustering.cluster_centers_[k]
        plt.plot(cluster_center[0], cluster_center[1], 'o',
                 markerfacecolor=color, markeredgecolor='k', markersize=6)
        plt.title("Example cluster allocation with a single random init\n"
                  "with MiniBatchKMeans")
    plt.show()








def getClusterFromGroup(grupoPropio,listaDeEtiquetas):
    grupo=-1
    print clustering.labels__[5]
    for i in clustering.labels_:
        grupo=grupo+1
        #if(grupo==grupoPropio)
    return grupo







"""imports RandomForest
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import GroupKFold
    from sklearn.cross_validation import KFold
"""



def estimador(comida,data):
    if (comida>0):
        return data['Carbohidratos'].median()
    else:
        return 0


"""drop de 'grupo' y clustering
    Esto deberia dropear la columa grupos pero no lo hace
    for index,grupo in data_agrupada:
        grupo=grupo.drop('Grupo',axis=1)
        print index
        print grupo
    print data_agrupada.get_group(82)
    cluster=KMeans()
    cluster.fit(data_agrupada)
"""


print "Prediciendo..."
o = alg.predict(output).astype(int)

print('Imprimiendo...')
submission = pd.DataFrame({
        "date": output["date"],
        "Historico": o
    })
submission.to_csv("output.csv", index = False)


print "Preparando IA..."
predictors=["date"]
target=["Historico"]
kf = KFold(data.shape[0], random_state=1)
alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)



"""CONVERTIR GROUPBY A ARRAY
grupos=[]
for index,grupo in data_agrupada:
    grupo=grupo.drop('Grupo',axis=1)
    grupos.append(grupo)
print grupos[0]
grupos=np.array(grupos, dtype=object)
cluster=KMeans()
#cluster.fit(data)
"""




"""
parser inicial
  columnas=['Hora','Tipo','Historico','Leida']
  data=pd.read_table('../csv.txt',header = 1, usecols=[1,2,3,4],names=columnas,parse_dates='Hora')
  print "Realizando adaptaciones pertinentes..."
  print "Esto puede tardar unos segundos"
  format="%Y/%m/%d %H:%M"
  primeraHora = data['Hora'].min()
  data['Grupo']=data['Hora'].map(lambda x: clasificaPorHora(x,primeraHora,format))
  """


gkf = GroupKFold(n_splits=3)

alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=8, min_samples_leaf=4)
# Compute the accuracy score for all the cross-validation folds; this is much simpler than what we did before
scores = cross_validation.cross_val_score(alg, data[predictors], data["Historico"], cv=3)

# Take the mean of the scores (because we have one for each fold)
print(scores.mean())

#output
columnas=['Hora']
output=pd.read_csv('../out.csv',header = 0, usecols=[0,1],names=columnas,parse_dates=True)
output['Grupo']=output['Hora'].map(lambda x: clasificaPorHora(x,"2016/03/30 16:30"))
output["date"]=output['Hora'].map(lambda x: pd.to_datetime(x,format=format, errors='ignore'))
output=output.drop('Hora',axis=1)
#output.groupby(by="Grupo").mean
#predicciones
train_data= data.values
test_data= output.values
predictors=["date","Tipo","Grupo"]
target=["Historico"]
# Initialize our algorithm class
alg = RandomForestClassifier(random_state=1, n_estimators=390, min_samples_split=16, min_samples_leaf=4)
# We set random_state to ensure we get the same splits every time we run this.
kf = KFold(data.shape[0], n_folds=3, random_state=1)
predictions = []
for train, test in kf:
    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.
    train_predictors = (data[predictors].iloc[train,:])
    # The target we're using to train the algorithm.
    train_target = data["Historico"].iloc[train]
    # Training the algorithm using the predictors and target.
    alg.fit(train_predictors, train_target)
    # We can now make predictions on the test fold
    test_predictions = alg.predict(data[predictors].iloc[test,:])
    predictions.append(test_predictions)
predictions = np.concatenate(predictions, axis=0)
print('imprimimos salida')
# submission--toCSV
submission = pandas.DataFrame({
        "date": output["date"],
        "Tipo": output["Tipo"],
        "Historico": predictions
    })
submission.to_csv("../put_out.csv", index = False)
